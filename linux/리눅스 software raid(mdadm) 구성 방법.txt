1. 환경 정보
CentOS Linux release 7.6.1810 (Core) 3.10.0-957.1.3.el7.x86_64
/dev/md0로 네이밍 예정
|- sdc1
|- sdd1
|- sde1
|- sdf1 각각 2TB씩 총 8TB. raid 5로 6TB 사용 예정.

2. 기존 레이드 구성 확인
[root@dev ~]# mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
        Raid Level : raid0
     Total Devices : 4
       Persistence : Superblock is persistent

             State : inactive
   Working Devices : 4

              Name : dev.bomsoftware.com:0  (local to host dev.bomsoftware.com)
              UUID : 2a0f839c:4d3fde79:fa982515:224bb034
            Events : 215589

    Number   Major   Minor   RaidDevice

       -       8       33        -        /dev/sdc1
       -       8       49        -        /dev/sdd1
       -       8       65        -        /dev/sde1
       -       8       81        -        /dev/sdf1

- 전원 작업으로 인하여 raid 구성이 깨진 상태(Superblock의 값을 못찾음)
- Number와 RaidDevice가 -로 표시됨

3. 마운트 된 기존 레이드 디스크 umount
[root@dev ~]# umount /dev/md0

4. mdadm 명령어로 기존 레이드 디스크 stop & remove
[root@dev ~]# mdadm -S /dev/md0
mdadm: stopped /dev/md0

[root@dev ~]# mdadm --remove /dev/md0

5. 대상 레이드 각각의 디스크 superblock 초기화
[root@dev ~]# mdadm --zero-superblock /dev/sdc1
[root@dev ~]# mdadm --zero-superblock /dev/sdd1
[root@dev ~]# mdadm --zero-superblock /dev/sde1
[root@dev ~]# mdadm --zero-superblock /dev/sdf1

6. 레이드 재 구성
[root@dev ~]# mdadm --create /dev/md0 --level=5 --raid-device=4 /dev/sdc1 /dev/sdd1 /dev/sde1 /dev/sdf1
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.

7. 레이드 구성 확인.
[root@dev ~]# mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Fri Feb 22 11:30:01 2019
        Raid Level : raid5
        Array Size : 5860144128 (5588.67 GiB 6000.79 GB)
     Used Dev Size : 1953381376 (1862.89 GiB 2000.26 GB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

     Intent Bitmap : Internal

       Update Time : Fri Feb 22 11:30:17 2019
             State : clean, degraded, recovering 
    Active Devices : 3
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 1

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : bitmap

    Rebuild Status : 0% complete

              Name : dev.bomsoftware.com:0  (local to host dev.bomsoftware.com)
              UUID : e49883e4:58257b1b:87b716d4:908c6c44
            Events : 4

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1
       2       8       65        2      active sync   /dev/sde1
       4       8       81        3      spare rebuilding   /dev/sdf1

- Raid Level과 Rebuild 내용이 정상적으로 확인됨.

[root@dev ~]# cat /proc/mdstat 
Personalities : [raid1] [raid6] [raid5] [raid4] 
md0 : active raid5 sdf1[4] sde1[2] sdd1[1] sdc1[0]
      5860144128 blocks super 1.2 level 5, 512k chunk, algorithm 2 [4/3] [UUU_]
      [=>...................]  recovery =  8.6% (169592708/1953381376) finish=157.6min speed=188615K/sec
      bitmap: 0/15 pages [0KB], 65536KB chunk

recovery와 finish 값을 통해 예상 종료시간 확인 가능.

lsblk로 디스크별 레이드 구성 확인 가능.
blkid로 디스크별 uuid 확인 가능

8. 레이드 구성 결과
[root@dev ~]# mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Fri Feb 22 11:30:01 2019
        Raid Level : raid5
        Array Size : 5860144128 (5588.67 GiB 6000.79 GB)
     Used Dev Size : 1953381376 (1862.89 GiB 2000.26 GB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

     Intent Bitmap : Internal

       Update Time : Fri Feb 22 15:11:28 2019
             State : clean 
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 0

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : bitmap

              Name : dev.bomsoftware.com:0  (local to host dev.bomsoftware.com)
              UUID : e49883e4:58257b1b:87b716d4:908c6c44
            Events : 2301

    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       49        1      active sync   /dev/sdd1
       2       8       65        2      active sync   /dev/sde1
       4       8       81        3      active sync   /dev/sdf1

[root@dev ~]# cat /proc/mdstat
Personalities : [raid1] [raid6] [raid5] [raid4] 
md0 : active raid5 sdf1[4] sde1[2] sdd1[1] sdc1[0]
      5860144128 blocks super 1.2 level 5, 512k chunk, algorithm 2 [4/4] [UUUU]
      bitmap: 0/15 pages [0KB], 65536KB chunk

unused devices: <none>

9. mdadm 설정 등록 (볼륨명 고정을 위해서)
[root@dev ~]# mdadm --detail --scan /dev/md0 >> /etc/mdadm.conf
ARRAY /dev/md0 metadata=1.2 name=dev.bomsoftware.com:0 UUID=e49883e4:58257b1b:87b716d4:908c6c44
이런식으로 디스크 볼륨명과 UUID가 있음.

10. 파일 시스템 포맷
[root@dev ~]# mkfs.ext4 /dev/md0
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=128 blocks, Stripe width=384 blocks
183132160 inodes, 1465036032 blocks
73251801 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=3613392896
44710 block groups
32768 blocks per group, 32768 fragments per group
4096 inodes per group
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 
	102400000, 214990848, 512000000, 550731776, 644972544

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done       

11. 디스크 마운트
[root@dev ~]# mount /dev/md0 /data

12. /etc/fstab 등록
[root@dev ~]# vi /etc/fstab
/dev/md0        /data    ext4    defaults,noatime,barrier=1,data=writeback    0    2

13. 확인
[root@dev ~]# mount | grep md0
/dev/md0 on /data type ext4 (rw,relatime,stripe=384,data=ordered)

